{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad604c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = 'cpu' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d73b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FOLDER_PATH = \"./archive/chest_xray/train/\"\n",
    "TEST_FOLDER_PATH = \"./archive/chest_xray/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9f77c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartiallyModifiedResNet18(nn.Module):\n",
    "    def __init__(self, n_classes, remove_layers=2, frozen_layers=6):\n",
    "        super(PartiallyModifiedResNet18, self).__init__()\n",
    "\n",
    "        # Load pre-trained ResNet-18 model\n",
    "        resnet18 = torchvision.models.resnet18(pretrained=True)\n",
    "        # Remove the specified number of layers from the end\n",
    "        resnet_layers = list(resnet18.children())[:-remove_layers]\n",
    "        #  last_resnet_layers = list(resnet18.children())[-2:]\n",
    "        self.features = nn.Sequential(*resnet_layers)\n",
    "\n",
    "        # Freeze the specified number of layers from the beginning\n",
    "        for i, param in enumerate(self.features.parameters()):\n",
    "            param.requires_grad = i < frozen_layers\n",
    "\n",
    "        # Modify the last fully connected layer\n",
    "        #  self.adaptive_poll = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=(7,7), stride=(1,1))\n",
    "        self.fc = nn.Linear(in_features=1*1*512, out_features=1000)\n",
    "        self.out = nn.Linear(in_features=1000, out_features=n_classes)\n",
    "        #  self.fc = nn.Linear(resnet18.fc.in_features, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.features(x)\n",
    "        #  x = self.adaptive_poll(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.relu(self.fc(x))\n",
    "        x = self.out(x)\n",
    "        #  x = x.view(x.size(0), -1)\n",
    "        #  x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6ef903c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset \n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "test_dataset = torchvision.datasets.ImageFolder(root = TEST_FOLDER_PATH, transform = train_transforms)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(root = TRAIN_FOLDER_PATH, transform = train_transforms)\n",
    "n_classes = len(set(train_dataset.targets))\n",
    "train, val = torch.utils.data.random_split(train_dataset, [4000, 5232-4000])\n",
    "train_loader = DataLoader(dataset=train, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5280f042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PartiallyModifiedResNet18(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (relu): ReLU()\n",
      "  (avgpool): AvgPool2d(kernel_size=(7, 7), stride=(1, 1), padding=0)\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  (out): Linear(in_features=1000, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/s/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURING MODEL PARAMETES\n",
    "\n",
    "n_batches = len(train_loader)\n",
    "\n",
    "model = PartiallyModifiedResNet18(n_classes=n_classes,remove_layers=7,frozen_layers=6).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c1fb1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0abe91d02f2e40e794f7542d4dd09ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x719104 and 512x1000)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/s/suraj/Resnet_Chest_Xray/resnet18_modified.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/s/suraj/Resnet_Chest_Xray/resnet18_modified.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/s/suraj/Resnet_Chest_Xray/resnet18_modified.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/s/suraj/Resnet_Chest_Xray/resnet18_modified.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m predictions \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/s/suraj/Resnet_Chest_Xray/resnet18_modified.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(predictions, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/s/suraj/Resnet_Chest_Xray/resnet18_modified.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m loss_list\u001b[39m.\u001b[39mappend(loss)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1518\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1519\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1528\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1525\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1527\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1528\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1530\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1531\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/s/suraj/Resnet_Chest_Xray/resnet18_modified.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/s/suraj/Resnet_Chest_Xray/resnet18_modified.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/s/suraj/Resnet_Chest_Xray/resnet18_modified.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mreshape(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/s/suraj/Resnet_Chest_Xray/resnet18_modified.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc(x))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/s/suraj/Resnet_Chest_Xray/resnet18_modified.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/s/suraj/Resnet_Chest_Xray/resnet18_modified.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1518\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1519\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1528\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1525\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1527\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1528\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1530\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1531\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x719104 and 512x1000)"
     ]
    }
   ],
   "source": [
    "# TRAINING:\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_list = []\n",
    "    num_correct, num_samples = 0, 0\n",
    "    for batch_num, (images, labels) in tqdm(enumerate(train_loader), total = n_batches):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        predictions = model(images)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss_list.append(loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, train_predictions = predictions.max(1)\n",
    "        num_correct += (train_predictions==labels).sum()\n",
    "        num_samples += predictions.size(0)\n",
    "    \n",
    "    training_accuracy = float(num_correct)/float(num_samples)*100 \n",
    "    \n",
    "    val_loss_list = []\n",
    "    val_num_correct, val_num_samples = 0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for val_batch_num, (val_images, val_labels) in enumerate(val_loader):\n",
    "            val_images = val_images.to(device)\n",
    "            val_labels = val_labels.to(device)\n",
    "\n",
    "            val_predictions = model(val_images)\n",
    "            _, val_preds = val_predictions.max(1)\n",
    "            \n",
    "            val_num_correct += (val_preds==val_labels).sum()\n",
    "            val_num_samples += val_preds.size(0)\n",
    "            \n",
    "            val_loss_list.append(criterion(val_predictions, val_labels))\n",
    "        \n",
    "    val_accuracy = float(val_num_correct)/float(val_num_samples)*100\n",
    "    model.train()\n",
    "\n",
    "    print(f'Epoch: {epoch + 1} | Accuracy: {training_accuracy:.2f} | loss: {sum(loss_list)/len(loss_list)}', end='')\n",
    "    print(f' | Validation Accuracy: {val_accuracy:.2f} | Validation loss: {sum(val_loss_list)/len(val_loss_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20340db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a test DataLoader similar to train and val DataLoader\n",
    "test_num_correct, test_num_samples = 0, 0\n",
    "test_loss_list = []\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for test_batch_num, (test_images, test_labels) in enumerate(test_loader):\n",
    "        test_images = test_images.to(device)\n",
    "        test_labels = test_labels.to(device)\n",
    "\n",
    "        test_predictions = model(test_images)\n",
    "        _, test_preds = test_predictions.max(1)\n",
    "\n",
    "        test_num_correct += (test_preds == test_labels).sum().item()\n",
    "        test_num_samples += test_preds.size(0)\n",
    "\n",
    "        test_loss_list.append(criterion(test_predictions, test_labels))\n",
    "\n",
    "test_accuracy = float(test_num_correct) / float(test_num_samples) * 100\n",
    "average_test_loss = sum(test_loss_list) / len(test_loss_list)\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy:.2f} | Test Loss: {average_test_loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
